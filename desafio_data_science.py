# -*- coding: utf-8 -*-
"""desafio_data_science.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dxFg1svf-BpHbD4qpGYGS8X2S_STD4S1

# DESAFIO
"""

# Montagem do drive para acessar dentro do colab
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.cluster import KMeans
from statsmodels.tsa.arima.model import ARIMA
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Ler o arquivo CSV e carregar em um dataframe
# Precisa passar o caminho corretamente
licitacao_2016 = pd.read_csv('../2016/licitacao.csv')
licitacao_2017 = pd.read_csv('../2017/licitacao.csv')
licitacao_2018 = pd.read_csv('../2018/licitacao.csv')
licitacao_2019 = pd.read_csv('../2019/licitacao.csv')

item_2016 = pd.read_csv('../2016/item.csv')
item_2017 = pd.read_csv('../2017/item.csv')
item_2018 = pd.read_csv('../2018/item.csv')
item_2019 = pd.read_csv('../2019/item.csv')

pd.set_option('display.max_columns', None)
licitacao_2016.head(10)

item_2016.head(10)

item_2016.info()

licitacao_2016.info()

df_licitacao = pd.concat([licitacao_2016, licitacao_2017, licitacao_2018, licitacao_2019])
df_item =  pd.concat([item_2016, item_2017, item_2018, item_2019])

"""Regressão Linear. O intercepto está muito baixo, recomendo verificar outro campo da base de dados ou até mesmo os dados."""

colunas = ['CD_ORGAO', 'ANO_LICITACAO', 'VL_LICITACAO']

df_regressao = df_licitacao.copy()

# Filtrar o dataframe apenas com as colunas selecionadas
dados_regressao = df_regressao[colunas].dropna()  # Remover linhas com valores ausentes

# Separar os dados de entrada (X) e de saída (y)
X = dados_regressao[['CD_ORGAO', 'ANO_LICITACAO']]  # Variáveis independentes
y = dados_regressao['VL_LICITACAO']  # Variável dependente

# Criar o modelo de regressão linear
model = LinearRegression()

# Treinar o modelo utilizando os dados
model.fit(X, y)

# Realizar previsões com base nos dados utilizados no treinamento
y_pred = model.predict(X)

# Exibir os resultados
print('Coeficientes:', model.coef_)
print('Intercepto:', model.intercept_)
print('R²:', model.score(X, y))

# Plotar os dados e a linha de regressão
plt.scatter(X['ANO_LICITACAO'], y, color='blue', label='Dados reais')
plt.plot(X['ANO_LICITACAO'], y_pred, color='red', label='Linha de regressão')
plt.xlabel('Ano Licitação')
plt.ylabel('Valor Licitação')
plt.legend()
plt.show()

# Cópia do dataframe
df_agrupamento = df_licitacao.copy()

"""Agrupamento realizado com o KNN, foi separado manualmente em 5 cluster, o correto seria fazer a verificação do "cotovelo" (método elbow ou coeficiente da silhueta média) para a definir de forma mais adequada a quantidade de cluster"""

# Selecionar colunas relevantes para o agrupamento
colunas = ['ANO_LICITACAO', 'VL_LICITACAO']
df_agrupamento = df_licitacao.copy()

# Retirado dados ausentes
df_agrupamento = df_agrupamento[colunas].dropna()

# Converter colunas numéricas para float
df_agrupamento[colunas] = df_agrupamento[colunas].astype(float)

# Criar o objeto scaler
scaler = MinMaxScaler()

# Aplicar a transformação nos dados
df_normalizado = scaler.fit_transform(df_agrupamento)

# Aplicar o algoritmo de agrupamento (K-means)
k = 5  # número de clusters desejado
kmeans = KMeans(n_clusters=k)
clusters = kmeans.fit_predict(df_normalizado)

# Inverter a escala dos dados para obter os valores originais
df_original = scaler.inverse_transform(df_normalizado)

# Criar um novo dataframe com os valores originais e o cluster atribuído
df_final = pd.DataFrame(df_original, columns=colunas)
df_final['ANO_LICITACAO'] = df_final['ANO_LICITACAO'].astype(int)

# Adicionar a coluna de cluster ao dataframe original
df_final['CLUSTER'] = clusters

# Imprimir 10 linhas do dataframe
df_final.head(10)



"""Correlação dos dados, só para entender melhor as váriaveis da base"""

df_item.corr()

df_licitacao.corr()



"""Respondendo as perguntas.

Quais são os principais bens materiais comprados pelos órgãos públicos do Rio Grande do Sul?
"""

df = df_licitacao.copy()

# Pré-processamento dos dados
df = df[['DS_OBJETO']]
df.dropna(inplace=True)

# Definição da lista de stopwords em português
stopwords = ['a', 'e', 'i', 'o', 'u', 'de', 'do', 'da', 'dos', 'das']

# Vetorização dos dados de texto
vectorizer = TfidfVectorizer(stop_words=stopwords)
X = vectorizer.fit_transform(df['DS_OBJETO'])

# Aplicação do algoritmo de agrupamento K-means
k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(X)

# Adicionar o cluster ao dataframe
df['Cluster'] = kmeans.labels_

# Contagem dos itens em cada cluster
clusters, counts = np.unique(kmeans.labels_, return_counts=True)

# Plotagem do gráfico de barras
plt.bar(clusters, counts)
plt.xlabel('Cluster')
plt.ylabel('Contagem')
plt.title('Principais bens materiais comprados pelos órgãos públicos do RS')

# Exibir os principais bens materiais em cada cluster
for cluster_id, count in zip(clusters, counts):
    print(f"Cluster {cluster_id}: {count} exemplos")
    cluster_samples = df[kmeans.labels_ == cluster_id].sample(min(count, 5))
    for sample in cluster_samples['DS_OBJETO']:
        print(f"- {sample}")
    print()

plt.show()

df.head(10)

"""Quais são os principais bens materiais comprados pelos órgãos públicos do Rio Grande do Sul, por ano?"""

df = df_licitacao.copy()


# Pré-processamento dos dados
df = df_licitacao[['ANO_LICITACAO', 'DS_OBJETO']]
df.dropna(inplace=True)

# Definição da lista de stopwords em português
stopwords = ['a', 'e', 'i', 'o', 'u', 'de', 'do', 'da', 'dos', 'das']

# Vetorização dos dados de texto
vectorizer = TfidfVectorizer(stop_words=stopwords)
X = vectorizer.fit_transform(df['DS_OBJETO'])

# Aplicação do algoritmo de agrupamento K-means para cada ano
k = 5
cluster_results = {}
for year in df['ANO_LICITACAO'].unique():
    X_year = X[df['ANO_LICITACAO'] == year]
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_year)
    cluster_results[year] = kmeans

# Contagem de licitações por ano e por cluster
cluster_counts = pd.DataFrame(columns=['Ano', 'Cluster', 'Contagem'])
for year, kmeans in cluster_results.items():
    labels = kmeans.labels_
    counts = np.bincount(labels)
    for cluster_id, count in enumerate(counts):
        cluster_counts = cluster_counts.append({'Ano': year, 'Cluster': cluster_id, 'Contagem': count}, ignore_index=True)

# Plotar gráfico de barras
pivot_table = cluster_counts.pivot(index='Ano', columns='Cluster', values='Contagem')
pivot_table.plot(kind='bar', stacked=True)
plt.xlabel('Ano')
plt.ylabel('Contagem de Licitações')
plt.title('Contagem de Licitações por Ano e Cluster')

plt.show()



"""É possível identificar algum padrão nas compras?"""

"""
# Dividir df_item em subconjuntos menores
chunk_size = 10000
chunks = [df_item[i:i+chunk_size] for i in range(0, len(df_item), chunk_size)]

# Realizar merge por partes
merged_df = pd.DataFrame()
for chunk in chunks:
    merged_chunk = chunk.merge(df_licitacao, on=["CD_ORGAO", "NR_LICITACAO", "ANO_LICITACAO"], how="inner")
    merged_df = pd.concat([merged_df, merged_chunk], ignore_index=True)
"""

df_item_sorted = df_item.sort_values(by=["CD_ORGAO", "NR_LICITACAO", "ANO_LICITACAO"])
df_licitacao_sorted = df_licitacao.sort_values(by=["CD_ORGAO", "NR_LICITACAO", "ANO_LICITACAO"])

# Corte do dataframe em quantidade de linhas para melhorar o desenpenho quando executar o algoritmo.
# Com o pandas a estouro de memória é recomendado utilizar o PySpark por exemplo.
df_item_subset = df_item_sorted[:30000]
df_licitacao_subset = df_licitacao_sorted[:30000]

merged_df = df_item_subset.merge(df_licitacao_subset, on=["CD_ORGAO", "NR_LICITACAO", "ANO_LICITACAO"], how="inner")

# Pré-processamento dos dados
df_processed = merged_df[["DS_ITEM", "VL_TOTAL_ESTIMADO"]].copy()
df_processed["DS_ITEM"] = df_processed["DS_ITEM"].fillna("")  # Trata valores ausentes
df_processed["VL_TOTAL_ESTIMADO"] = df_processed["VL_TOTAL_ESTIMADO"].fillna(0)  # Trata valores ausentes

# Codificação da variável categórica "DS_ITEM"
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df_processed["DS_ITEM"])

# Normalização dos dados numéricos
scaler = MinMaxScaler()
df_processed["VL_TOTAL_ESTIMADO"] = scaler.fit_transform(df_processed[["VL_TOTAL_ESTIMADO"]])

# Aplicação do algoritmo de clusterização K-means
k = 5
kmeans = KMeans(n_clusters=k)
kmeans.fit(X)

df_processed["VL_TOTAL_ESTIMADO"] = scaler.inverse_transform(df_processed[["VL_TOTAL_ESTIMADO"]])

df_processed["Cluster"] = kmeans.labels_

plt.scatter(df_processed["VL_TOTAL_ESTIMADO"], df_processed["Cluster"])
plt.xlabel("VL_TOTAL_ESTIMADO")
plt.ylabel("Cluster")
plt.title("Padrões nas Compras")
plt.show()



df_combined = merged_df.copy()

# Codificar as variáveis categóricas, pode se utilizar váriaveis dummy pelo pandas, get_dummy()
label_encoder = LabelEncoder()
df_combined['NM_ORGAO'] = label_encoder.fit_transform(df_combined['NM_ORGAO'])

# Dividir os dados em treinamento e teste
X = df_combined[['ANO_LICITACAO', 'CD_ORGAO']]
y = df_combined['NM_ORGAO']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinar um modelo de classificação
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Fazer previsões no conjunto de teste
y_pred = model.predict(X_test)

# Avaliar o desempenho do modelo
report = classification_report(y_test, y_pred)
print(report)

# Plotar o gráfico de pizza
class_names = label_encoder.classes_
class_counts = df_combined['NM_ORGAO'].value_counts().sort_index()
class_counts_str = [str(count) for count in class_counts]
plt.pie(class_counts, labels=class_names, autopct='%1.1f%%')
plt.title('Distribuição das Classes')
plt.show()



class_counts_by_year = df_combined.groupby(['ANO_LICITACAO', 'NM_ORGAO']).size().unstack(fill_value=0)

# Plotar os gráficos separados por ano
for year in class_counts_by_year.index:
    class_counts = class_counts_by_year.loc[year]
    class_names = label_encoder.classes_

    class_counts_str = [str(count) for count in class_counts]

    fig, ax = plt.subplots()
    ax.pie(class_counts, labels=class_names, autopct='%1.1f%%')
    ax.set_title(f'Distribuição das Classes - Ano {year}')
    ax.annotate(str(year), (0, 0), fontsize=12, fontweight='bold', va='center', ha='center')
    plt.legend(class_names, title='Órgão', loc='center left', bbox_to_anchor=(1, 0.5))
    plt.show()



"""Previsão da variável Valor Unitário Homologado para o ano de 2020, sem tratamento de Outlier."""

# Preparação dos dados
df_item_previsao = df_item[['ANO_LICITACAO', 'VL_UNITARIO_HOMOLOGADO']].copy()
df_agrupado = df_item_previsao.groupby('ANO_LICITACAO').sum()
serie_temporal = pd.Series(df_agrupado['VL_UNITARIO_HOMOLOGADO'])

# Aplicação do modelo ARIMA para a previsão
ordem_arima = (1, 1, 1)
modelo = ARIMA(serie_temporal, order=ordem_arima)
resultado = modelo.fit()

# Previsão para o próximo ano
predicao = resultado.predict(start=len(serie_temporal), end=len(serie_temporal), typ='levels')
ultimo_ano = serie_temporal.index[-1]

# Gráfico de barras com os valores observados
plt.bar(serie_temporal.index, serie_temporal, label='Valores Observados')

# Gráfico de barras com os valores preditos
plt.bar([ultimo_ano+1], predicao, color='red', label='Valor Preditos')

plt.xlabel('Ano')
plt.ylabel('Total Estimado')
plt.title('Previsão de Séries Temporais')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.show()

